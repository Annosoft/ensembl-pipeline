========================================================================
SETTING UP TESTS
========================================================================

In the pipeline testing system, no matter if a single analysis is tested
or the whole pipeline is tested, all analyses are run on genomic DNA
sequences.

This document introduces the key data components of the test setup.

Currently, all tests run on the first 10 Mb of human chromosome 1
(NCBI36).

Note, the various scripts distributed here will assume
that the $TESTROOT environment variable points to the
ensembl-pipeline/test_system directory, or to a copy of that directory.

------------------------------------------------------------------------

Depending on the actual analysis(es) tested, a certain amount of data
needs to be available to allow RUNNING of the analyses in the first
place:

(1) The input data your test requires must be available.  This means the
standard "sequence tables" (a.k.a. the core sequence tables) need to
have data before anything else can run.  The sequence tables are:

  analysis
  assembly
  attrib_type
  coord_system
  dna
  meta
  meta_coord
  seq_region,
  seq_region_attrib

Four "pipeline tables" also need to have data prior to running
the test.  The pipeline tables are:

  rule_goal
  rule_conditions
  input_id_type_analysis
  input_id_analysis

The tables rule_goal and rule_condition are only important when running
the full pipeline test because single analyses tests don't use the rule
tables.  However, the input_id_type_analysis and input_id_analysis
tables ALWAYS need to be filled out as this is where input IDs are
generated from when the test for a single analysis or for the whole
pipeline is run.

If your analyses require input from a file you will need to make sure
that the file is also available somewhere.  For example, if running
the test for the "marker" analysis, a text file containing STS marker
data (forward and reverse primer sequences, expected E-PCR product
size, etc) must be available.  Or, if running the test for the "pmatch"
analysis, a text file containing FASTA sequences of query proteins must
be available.

The core tables, pipeline tables and other required data files (such as
the marker and protein files mentioned above) for running test on the
first 10 Mb of human chromosome 1 are already available in a compressed
archive (homo_sapiens.zip) in

  ensembl-pipeline/test_system/reference_data/

or, if the $TESTROOT environment variable is set, in

  $TESTROOT/reference_data/

The marker and protein files are stored in the archive in the "data"
directory (see also point 3 below).

(2) Prerequisite data needs to be available before an analysis may be
run.  For example, when running the test for the "vertrna" analysis, as
it is BLASTing EMBL Vertrna sequences (the "queries") against Genscan
hits (the "targets", generated by the "genscan" analysis), Genscan data,
which are found in prediction_exon and prediction_transcript tables,
must be available.

Below is a quick guide to the tables (other than core and pipeline)
required by specific analyses:

  ANALYSIS      TABLES TO BE FILLED PRIOR TO RUNNING ANALYSIS
  ----------------------------------------------------------------------

  firstef       repeat_feature, repeat_consensus

  genscan       repeat_feature, repeat_consensus

  uniprot       repeat_feature, repeat_consensus, prediction_exon,
                prediction_transcript

  vertrna       repeat_feature, repeat_consensus, prediction_exon,
                prediction_transcript

  unigene       repeat_feature, repeat_consensus, prediction_exon,
                prediction_transcript

  marker        marker, map, marker_synonym, marker_map_location

  bestpmatch    protein_align_feature.Pmatch


Again, data for the required tables have already been dumped out and are
available in

  $TESTROOT/reference_data/homo_sapiens.zip

(See point 3 below for more info.)

(3) A reference set of results must be availible.

The set serves two purposes: first, as a source of data for the core,
pipeline and other tables, so as to allow analyses to run from the
"test" database, (as mentioned in points 1 and 2 above); second, as a
source of "standard" data if the test output is to be compared against a
reference to be stored in the "reference" database.

Note that the "test" database holds data to run an analysis or
the entire pipeline and does not contain any results of the
analysis/analyses at the beginning of the test.  The "reference"
database on the other hand contains the "expected" results for the
analysis/analyses tested right from the start as the results were loaded
into "reference" database from a series of text files.

The reference data set usually comes from a small slice of the genome
and is hence quite small, e.g., data from the raw compute, Pmatch and
marker analyses for the first 10 Mb of human chromosome 1.  As already
has been mentioned, this data may be found here:

  $TESTROOT/reference_data/homo_sapiens.zip


(3.1) How can I generate reference data set from scratch?

If the reference data set needs to be created from scratch, you can dump
out the required data on the basis of a slice by using this script:

  $TESTROOT/setup/slice_to_sql.pl

With this script, you can give information about a slice, the slice's
coord_system_name, version, seq_region_name, seq_region_start,
seq_region_end, etc. and a list of required tables, and it will dump the
corresponding information from those tables for that slice in the form
of tab-delimited text files.

The script uses the module Bio::EnsEMBL::Pipeline::Utils::SliceDump.  In
the module, for every table which can be dumped on the basis of a slice,
there is a method which fits the format "dump_<table_name>_table".
These methods then know how to dump *only* the information which is
relevant to the specified slice.  The tables are dumped into files
with the name format "<table_name>.<slice_name>".  As a result, very
often the data dumped out from a "partial" table will be contained in
hundreds of files (e.g., for the seq_region table, you might end up
having "seq_region.contig1", "seq_region.contig2", "seq_region.contig3",
and so on).  Concatenate these individual files to "regenerate" the
"proper" partial table (i.e "seq_region").  Please refer to Perldoc in
$TESTROOT/setup/slice_to_sql.pl for more details.

The slice_to_sql.pl script will also know if a table cannot be dumped
partially based on a specific slice; in this case the script will dump
out the whole table, again in a tab-delimited text file.

Make sure the final set of text files to be used as reference data
are named after the tables from which the data came from.  The file
extension will not be taken as part of the table name as it is
stripped off by mysqlimport during import.  E.g. text files named
"protein_align_feature.Pmatch" and "protein_align_feature" will both
have data loaded from the protein_align_feature table (see point 3.2
below for more info).

The dumped text files should then be achieved in a compressed file named
after the species, e.g., "homo_sapiens.zip", and stored in

  $TESTROOT/reference_data/

(3.2) How are the dumped out text files used in the tests?

When the system test is run, either using the test_single_analysis.pl or
test_whole_pipeline.pl script, at some point the load_<filename>_tables
method in the TestDB.pm module will be called, and the dumped text files
will be loaded into the "test" and "reference" databases by mysqlimport
commands.

The TestDB module expects any data it is to load to live in a compressed
archive with a name indicating the species (e.g., "homo_sapiens.zip").
It also assumes that the files in the compressed archive are named
after the relevant tables.  (Hence the need to keep reference data as a
compressed archive in the $TESTROOT/reference_data/ directory).

The TestDB.pm module loads tables when given a list of tables to
load (using the method "load_tables") or, when given the name of a
table group, loads a predefined list of tables (using method the
load_<table_group>_tables).  The latter allows easier loading of certain
sets of tables.  For example, the method "load_core_tables" contains a
predefined list of core tables which always need to be loaded for any
test.

==> A note about the "load_tables" methods and stripping of text file
    extensions:

These methods are called by an array of text file names (with
extension), not table names. So the array can include text file
names like:"protein_align_feature.Pmatch" or "simple_feature.CpG" or
"dna_align_feature.Vertrna", and file names like "simple_feature.CpG"
(CpG data in simple_feature table) and "simple_feature.Eponine" (Eponine
data in simple_feature table) can coexist in the array.

The method will retrieve data from each of the text file, THEN strips
off the file extension (done by mysqlimport) and write the data into the
required tables.  In the example of loading "simple_feature.CpG" and
"simple_feature.Eponine" files, the method will NOT be loading data from
"simple_feature" text file, but instead loading data specifically from
CpG and Eponine analyses only.

========================================================================

In summary, to set up a test for a specific analysis, you need:

  1. an entry in an analysis table (which is one of the core tables);

  2. an entry in the rule tables (if to be tested as part of the
     complete pipeline);

  3. sequence data for the database (available in the core tables);

  4. some input data, this is sequence data for most analyses but it may
     also be, e.g., a FASTA file of protein sequences;

  5. correct configuration (an entry in BatchQueue.pm plus any analysis
     specific config, see doc "running_tests.txt" for details);

  6. reference data containing expected results of analyses if the test
     results are to be compared against a standard reference.
