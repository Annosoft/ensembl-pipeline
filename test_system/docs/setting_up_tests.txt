================
SETTING UP TESTS
================

In the pipeline testing system, no matter if a single analysis is tested or
the whole pipeline is tested, all analyses are run on genomic DNA sequences.

This document introduces the key data components of the test setup.

Currently all tests run on human chr 1 1-10000000bp (i.e. the first 10Mb).

--------------------------------------------------------------------------------

Depending on the actual analysis(es) tested, a certain amount of data needs to be 
availible to allow RUNNING of the analyses in the first place:

(1) The input data your test requires must be available. 

This means the standard sequence ("core") tables need to have data before 
anything else can run. The standard tables are:

meta, meta_coord, coord_system, analysis, attrib_type, seq_region,
assembly, dna and seq_region_attrib.

The four pipeline tables also need to have data prior to running the test.
The pipeline tables are: rule_goal, rule_conditions, input_id_type_analysis 
and input_id_analysis. rule_goal and rule_condition are only important for the 
whole pipeline test because single analyses tests don't use the rule table. 
However, the input_id_type_analysis and input_id_analysis tables ALWAYS need 
to be filled out as this is where input_ids are generated from when the test 
for a single analysis or for the whole pipeline is run.

If your analyses require input from a file you will need to make sure that the
file is also availble somewhere. For example, if running test for the "marker"
analysis, a text file containing STS marker data (fwd and rev primer sequences,
expected E-PCR product size, etc) must be available.  Or, if running test for
the "Pmatch" analysis, a text file containing FASTA sequences of query proteins
must be available.

The core tables, pipeline tables and other required data files (such 
as the marker and protein files mentioned above) for running test on human chr 1, 
1-10Mb are already available in a zipped file (homo_sapiens.zip) in 
ensembl-pipeline/test_system/reference_data/. The marker/protein files are
stored in homo_sapiens.zip, under the "data" directory (See also point 3 below.)


(2) Database tables the analysis relies on being filled must also have 
data availible. Here we're referring to the tables other than the core and 
pipeline ones mentioned in point (1) above. For example, when running test 
for the "Vertrna" analysis, as it's BLASTing EMBL Vertrna DB sequences (the 
"queries") against Genscan hits (the "targets", generated by the Genscan analysis), 
Genscan data, which are found in prediction_exon and prediction_transcript tables, 
must be available.                                                                              

Below is a quick guide to the tables (other than core and pipeline) required by 
specific analyses:

ANALYSIS                  TABLES TO BE FILLED PRIOR TO RUNNING ANALYSIS
--------        ------------------------------------------------------------------
FirstEF         'repeat_feature', 'repeat_consensus'
Genscan         'repeat_feature', 'repeat_consensus'
Uniprot         'repeat_feature', 'repeat_consensus', 'prediction_exon', 
                'prediction_transcript'
Vertrna         'repeat_feature', 'repeat_consensus', 'prediction_exon', 
                'prediction_transcript'
Unigene         'repeat_feature', 'repeat_consensus', 'prediction_exon', 
                'prediction_transcript' 
marker          'marker', 'map', 'marker_synonym', 'marker_map_location'
BestPmatch      'protein_align_feature.Pmatch'


Again, data for the required tables have already been dumped out and 
are available in ensembl-pipeline/test_system/reference_data/homo_sapiens.zip.
(See point 3 below for more info.)


(3) A reference set of results must be availible. 

The set serves two purposes: first, as a source of data for the core, pipeline 
and other tables, so as to allow analyses to run from the "test" DB, (as mentioned
in points 1 and 2 above); second, as a source of "standard" data if the test 
output is to be compared against a reference to be stored in the "reference" DB.

N.B. The "test" DB holds data to run an analysis or the entire pipeline and 
does not contain any results of the analysis/analyses at the beginning of 
the test. The "reference" DB, in contrast, contains the "expected" results for 
the analysis/analyses tested right from the start as the results were loaded into
"reference" DB from a series of text files.
 
The reference data set usually comes from a small slice of the genome and is hence
quite small, e.g. data from raw compute, Pmatch and marker analyses for the first 
10Mb of human chromosome 1. The human Chr1 1-10Mb ref data can be found at:

ensembl-pipeline/test_system/reference_data/homo_sapiens.zip


(3.1) How can I generate reference data set from scratch?

If the reference data set needs to be created from scratch, you can dump out 
the required data on the basis of a slice by using this script:

ensembl-pipeline/test_system/setup/slice_to_sql.pl

With this script, you can give information about a slice, the slice's 
coord_system_name, version, seq_region_name, seq_region_start/end etc and 
a list of required tables, and it will dump the corresponding information 
from those tables for that slice in the form of tab-delimited text files. 

This script uses the module Bio::EnsEMBL::Pipeline::Utils::SliceDump. In the
module, for every table which can be dumped on the basis of a slice, there is
a method which fits the format dump_*table_name*_table. These methods
then know how to dump *only* the information which is relevant to the 
specified slice. The tables are dumped into files with the name format
table_name.slice_name. As a result, very often the data dumped out from a 
"partial" table will be contained in hundreds of files (e.g. for the seq_region
table, you might end up having seq_region.contig1, seq_region.contig2, 
seq_region.contig3, and so on). Concatenate these individual files to "regenerate" 
the "proper" partial table (i.e "seq_region"). Please refer to Perldoc in 
ensembl-pipeline/test_system/setup/slice_to_sql.pl for more details.

The slice_to_sql.pl script also will know if a table cannot be dumped
partially based on a specific slice; in this case the script will dump out
the whole table, again in a tab-delimited text file.

Make sure the final set of text files to be used as reference data are named 
after the tables from which the data came from. The file extension will not be
taken as part of the table name as it is stripped off by mysqlimport during import. 
E.g. text files named "protein_align_feature.Pmatch" and "protein_align_feature"
will both have data loaded into "protein_align_feature" table. (see point 3.2 below 
for more info).  
        
The dumped text files should then be zipped up (file named after a species, 
e.g. homo_sapiens.zip) and stored in:

ensembl-pipeline/test_system/reference_data.


(3.2) How are the dumped out text files used in the tests?

When the system test is run, either using the test_single_analysis.pl or 
test_whole_pipeline.pl script, at some point the load_*filename*_tables method
in the TestDB.pm module will be called, and the dumped text files will be loaded 
into the "test" and "reference" databases by mysqlimport commands.

The TestDB.pm module expects any data it is to load to live in a zipped 
file named after a species (e.g. homo_sapiens.zip) and for that the files in 
the zipped file to be named table_name.  (Hence the need to keep reference data 
as a zipped file in ensembl-pipeline/test_system/reference_data).

The TestDB.pm module loads tables when given a list of tables to load (using method:
"load_tables") or, when given the name of a table group, loads a 
predefined list of tables (using method: load_*table_group*_tables). The latter 
allows easier loading of certain sets of tables. For example, the method 
"load_core_tables" contains a predefined list of core tables which
always need to be loaded for any test.

**A note about the "load_tables" methods and stripping of text file extensions:

These methods are called by an array of text file names (with extension), not table 
names. So the array can include text file names like:"protein_align_feature.Pmatch" 
or "simple_feature.CpG" or "dna_align_feature.Vertrna", and file names like
"simple_feature.CpG" (CpG data in simple_feature table) and "simple_feature.Eponine" 
(Eponine data in simple_feature table) can coexist in the array.

The method will retrieve data from each of the text file, THEN strips off the file 
extension (done by mysqlimport) and write the data into the required tables.  In the 
example of loading "simple_feature.CpG" and "simple_feature.Eponine" files, the method 
will NOT be loading data from "simple_feature" text file, but instead loading data 
specifically from CpG and Eponine analyses only.

========================================================================================

In summary, to setup a test for a specific analysis, you need:

1. an entry in an analysis table (which is one of the core tables);

2. an entry in the rule tables (if to be tested as part of the 
complete pipeline);

3. sequence data for the database (available in the core tables);

4. some input data, this is sequence data for most analyses but it may also
be e.g. a FASTA file of protein sequences;

5. correct configuration (an entry in BatchQueue.pm plus any analysis 
specific config, see doc "running_tests.txt" for details);

6. reference data containing expected results of analyses if the test results 
are to be compared against a standard reference.
